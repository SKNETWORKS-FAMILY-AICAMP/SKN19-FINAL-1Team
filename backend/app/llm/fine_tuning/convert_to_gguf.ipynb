{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd880b8f-d2f8-45dd-af4f-0239b3cf6741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess\n",
    "from huggingface_hub import HfApi, login, snapshot_download\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# ë³€í™˜í•  ì›ë³¸ ëª¨ë¸\n",
    "SOURCE_MODEL_ID = \"\" \n",
    "\n",
    "# ì—…ë¡œë“œí•  ë¦¬í¬ì§€í† ë¦¬\n",
    "TARGET_REPO_ID = \"\" \n",
    "\n",
    "# íŒŒì¼ëª… ì„¤ì •\n",
    "GGUF_FILENAME_FP16 = \"model-f16.gguf\"\n",
    "GGUF_FILENAME_Q4 = \"model-Q4_K_M.gguf\"\n",
    "\n",
    "BASE_DIR = os.path.abspath(\".\")\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"downloaded_model\")\n",
    "\n",
    "\n",
    "LLAMA_CPP_DIR = os.path.join(BASE_DIR, \"llama.cpp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0f0724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command(command, cwd=None):\n",
    "    print(f\"ì‹¤í–‰: {' '.join(command)}\")\n",
    "    try:\n",
    "        subprocess.run(command, check=True, cwd=cwd)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad50ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    if not HF_TOKEN:\n",
    "        print(\"ì˜¤ë¥˜: HF_TOKEN ì—†ìŒ\")\n",
    "        sys.exit(1)\n",
    "    login(token=HF_TOKEN)\n",
    "    \n",
    "    print(f\"ì›ë³¸ ëª¨ë¸: {SOURCE_MODEL_ID}\")\n",
    "    print(f\"íƒ€ê²Ÿ ë¦¬í¬ì§€í† ë¦¬: {TARGET_REPO_ID}\")\n",
    "\n",
    "    print(f\"\\nëª¨ë¸ ë‹¤ìš´ë¡œë“œ: ({SOURCE_MODEL_ID})\")\n",
    "    if os.path.exists(MODEL_DIR):\n",
    "        shutil.rmtree(MODEL_DIR)\n",
    "        \n",
    "    try:\n",
    "        snapshot_download(\n",
    "            repo_id=SOURCE_MODEL_ID,\n",
    "            local_dir=MODEL_DIR,\n",
    "            local_dir_use_symlinks=False,\n",
    "            ignore_patterns=[\"*.gguf\", \"*.bin\", \"*.pth\"]\n",
    "        )\n",
    "        print(\"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\")\n",
    "    except Exception as e:\n",
    "        print(f\"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216200d2-7521-4664-a048-70ff3f9cec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if not os.path.exists(LLAMA_CPP_DIR):\n",
    "        run_command([\"git\", \"clone\", \"https://github.com/ggerganov/llama.cpp.git\"])\n",
    "    \n",
    "    # í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "    run_command([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"], cwd=LLAMA_CPP_DIR)\n",
    "    \n",
    "    # CMake ë¹Œë“œ (ì–‘ìí™” ë„êµ¬ ìƒì„±)\n",
    "    print(\"llama.cpp ë¹Œë“œ ì¤‘...\")\n",
    "    build_dir = os.path.join(LLAMA_CPP_DIR, \"build\")\n",
    "    run_command([\"cmake\", \"-B\", build_dir, \"-DGGML_NATIVE=OFF\"], cwd=LLAMA_CPP_DIR)\n",
    "    run_command([\"cmake\", \"--build\", build_dir, \"--config\", \"Release\", \"-j\", \"4\"], cwd=LLAMA_CPP_DIR)\n",
    "    \n",
    "    # ì–‘ìí™” ì‹¤í–‰ íŒŒì¼ ìœ„ì¹˜ ì°¾ê¸°\n",
    "    quantize_bin = os.path.join(build_dir, \"bin\", \"llama-quantize\")\n",
    "    if not os.path.exists(quantize_bin):\n",
    "        # ê²½ë¡œê°€ ë‹¤ë¥¼ ê²½ìš° (ë£¨íŠ¸ ë¹Œë“œ ë“±) ëŒ€ë¹„\n",
    "        quantize_bin = os.path.join(build_dir, \"llama-quantize\")\n",
    "        \n",
    "    if not os.path.exists(quantize_bin):\n",
    "        print(\"âŒ ì˜¤ë¥˜: llama-quantize ë°”ì´ë„ˆë¦¬ ë¹Œë“œ ì‹¤íŒ¨.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(f\"\\n[3/5] GGUF ë³€í™˜ (FP16) ìˆ˜í–‰ ì¤‘...\")\n",
    "    convert_script = os.path.join(LLAMA_CPP_DIR, \"convert_hf_to_gguf.py\")\n",
    "    \n",
    "    run_command([\n",
    "        sys.executable, convert_script,\n",
    "        MODEL_DIR,\n",
    "        \"--outfile\", GGUF_FILENAME_FP16,\n",
    "        \"--outtype\", \"f16\"\n",
    "    ])\n",
    "    \n",
    "    if not os.path.exists(GGUF_FILENAME_FP16):\n",
    "        print(\"âŒ ë³€í™˜ ì‹¤íŒ¨:\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(f\"\\n4bit ì–‘ìí™”\")\n",
    "    \n",
    "    run_command([\n",
    "        quantize_bin,\n",
    "        GGUF_FILENAME_FP16,\n",
    "        GGUF_FILENAME_Q4,\n",
    "        \"Q4_K_M\"\n",
    "    ])\n",
    "    \n",
    "    if not os.path.exists(GGUF_FILENAME_Q4):\n",
    "        print(\"âŒ ì–‘ìí™” ì‹¤íŒ¨\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    print(f\"âœ… ì–‘ìí™” ì™„ë£Œ: {GGUF_FILENAME_Q4}\")\n",
    "    \n",
    "    # ìš©ëŸ‰ í™•ë³´ë¥¼ ìœ„í•´ FP16 íŒŒì¼ ì‚­ì œ\n",
    "    os.remove(GGUF_FILENAME_FP16)\n",
    "    shutil.rmtree(MODEL_DIR) \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581897bb-71bc-4a97-afd3-2a338256a6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/5] Hugging Face ì—…ë¡œë“œ ì‹œì‘ (WindyAle/Kanana-nano-2.1B-Finance-v1-GGUF)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f9b8f4fef74eaa96e84597b87aba8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cddcdb1503cd461e813856777a72fe39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "ğŸ”— ëª¨ë¸ ë§í¬: https://huggingface.co/WindyAle/Kanana-nano-2.1B-Finance-v1-GGUF\n"
     ]
    }
   ],
   "source": [
    "# ì—…ë¡œë“œ\n",
    "print(f\"\\n[5/5] Hugging Face ì—…ë¡œë“œ ì‹œì‘ ({TARGET_REPO_ID})...\")\n",
    "login(token=HF_TOKEN)    \n",
    "api = HfApi()\n",
    "\n",
    "api.create_repo(repo_id=TARGET_REPO_ID, exist_ok=True, repo_type=\"model\")\n",
    "\n",
    "try:\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=GGUF_FILENAME_Q4,\n",
    "        path_in_repo=GGUF_FILENAME_Q4,\n",
    "        repo_id=TARGET_REPO_ID,\n",
    "        repo_type=\"model\"\n",
    "    )\n",
    "    print(\"\\nì—…ë¡œë“œ ì™„ë£Œ\")\n",
    "    print(f\"ëª¨ë¸ ë§í¬: https://huggingface.co/{TARGET_REPO_ID}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
