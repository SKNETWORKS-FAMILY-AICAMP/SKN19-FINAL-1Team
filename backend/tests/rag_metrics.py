"""
RAG í‰ê°€ ì§€í‘œ ëª¨ë“ˆ

í¬í•¨ëœ ì§€í‘œ:
1. ê²€ìƒ‰ í’ˆì§ˆ ì§€í‘œ: Precision@K, Recall@K, MRR, NDCG, Hit Rate
2. RAGAS í”„ë ˆì„ì›Œí¬: Faithfulness, Answer Relevancy, Context Precision
3. ì‹œë§¨í‹± ìœ ì‚¬ë„: Cosine Similarity
"""

import math
import os
import sys
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Tuple
import numpy as np

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dotenv import load_dotenv
load_dotenv()

from app.llm.base import get_openai_client


# ============================================================================
# 1. ê²€ìƒ‰ í’ˆì§ˆ ì§€í‘œ (Retrieval Quality Metrics)
# ============================================================================

@dataclass
class RetrievalMetrics:
    """ê²€ìƒ‰ í’ˆì§ˆ ì§€í‘œ ê²°ê³¼"""
    precision_at_k: float = 0.0      # Top-K ì¤‘ ê´€ë ¨ ë¬¸ì„œ ë¹„ìœ¨
    recall_at_k: float = 0.0         # ì „ì²´ ê´€ë ¨ ë¬¸ì„œ ì¤‘ ê²€ìƒ‰ëœ ë¹„ìœ¨
    mrr: float = 0.0                 # Mean Reciprocal Rank
    ndcg_at_k: float = 0.0           # Normalized DCG
    hit_rate: float = 0.0            # Top-Kì— ê´€ë ¨ ë¬¸ì„œ ìˆëŠ”ì§€


def precision_at_k(retrieved_docs: List[Dict], relevant_ids: set, k: int = 5) -> float:
    """
    Precision@K: Top-K ê²€ìƒ‰ ê²°ê³¼ ì¤‘ ê´€ë ¨ ë¬¸ì„œì˜ ë¹„ìœ¨

    Args:
        retrieved_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ìˆœìœ„ìˆœ)
        relevant_ids: ê´€ë ¨ ë¬¸ì„œ ID set
        k: ìƒìœ„ Kê°œ

    Returns:
        Precision@K ê°’ (0~1)
    """
    if not retrieved_docs or k <= 0:
        return 0.0

    top_k = retrieved_docs[:k]
    relevant_count = sum(1 for doc in top_k if doc.get("id") in relevant_ids)
    return relevant_count / k


def recall_at_k(retrieved_docs: List[Dict], relevant_ids: set, k: int = 5) -> float:
    """
    Recall@K: ì „ì²´ ê´€ë ¨ ë¬¸ì„œ ì¤‘ Top-Kì— í¬í•¨ëœ ë¹„ìœ¨

    Args:
        retrieved_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ìˆœìœ„ìˆœ)
        relevant_ids: ê´€ë ¨ ë¬¸ì„œ ID set
        k: ìƒìœ„ Kê°œ

    Returns:
        Recall@K ê°’ (0~1)
    """
    if not relevant_ids:
        return 0.0

    top_k = retrieved_docs[:k]
    retrieved_relevant = sum(1 for doc in top_k if doc.get("id") in relevant_ids)
    return retrieved_relevant / len(relevant_ids)


def mean_reciprocal_rank(retrieved_docs: List[Dict], relevant_ids: set) -> float:
    """
    MRR (Mean Reciprocal Rank): ì²« ë²ˆì§¸ ê´€ë ¨ ë¬¸ì„œì˜ ì—­ìˆœìœ„

    Args:
        retrieved_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ìˆœìœ„ìˆœ)
        relevant_ids: ê´€ë ¨ ë¬¸ì„œ ID set

    Returns:
        MRR ê°’ (0~1)
    """
    for i, doc in enumerate(retrieved_docs):
        if doc.get("id") in relevant_ids:
            return 1.0 / (i + 1)
    return 0.0


def dcg_at_k(relevance_scores: List[float], k: int) -> float:
    """DCG (Discounted Cumulative Gain) ê³„ì‚°"""
    dcg = 0.0
    for i, rel in enumerate(relevance_scores[:k]):
        dcg += rel / math.log2(i + 2)  # i+2 because log2(1) = 0
    return dcg


def ndcg_at_k(retrieved_docs: List[Dict], relevance_map: Dict[str, float], k: int = 5) -> float:
    """
    NDCG@K (Normalized Discounted Cumulative Gain)
    ìˆœìœ„ë¥¼ ê³ ë ¤í•œ ê²€ìƒ‰ í’ˆì§ˆ ì ìˆ˜

    Args:
        retrieved_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ìˆœìœ„ìˆœ)
        relevance_map: ë¬¸ì„œ ID -> ê´€ë ¨ë„ ì ìˆ˜ (0~1)
        k: ìƒìœ„ Kê°œ

    Returns:
        NDCG@K ê°’ (0~1)
    """
    if not retrieved_docs or not relevance_map:
        return 0.0

    # ì‹¤ì œ DCG ê³„ì‚°
    actual_scores = [relevance_map.get(doc.get("id"), 0.0) for doc in retrieved_docs[:k]]
    actual_dcg = dcg_at_k(actual_scores, k)

    # ì´ìƒì ì¸ DCG ê³„ì‚° (ê´€ë ¨ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬)
    ideal_scores = sorted(relevance_map.values(), reverse=True)[:k]
    ideal_dcg = dcg_at_k(ideal_scores, k)

    if ideal_dcg == 0:
        return 0.0

    return actual_dcg / ideal_dcg


def hit_rate(retrieved_docs: List[Dict], relevant_ids: set, k: int = 5) -> float:
    """
    Hit Rate: Top-Kì— ê´€ë ¨ ë¬¸ì„œê°€ í•˜ë‚˜ë¼ë„ ìˆëŠ”ì§€

    Returns:
        1.0 (hit) or 0.0 (miss)
    """
    top_k = retrieved_docs[:k]
    for doc in top_k:
        if doc.get("id") in relevant_ids:
            return 1.0
    return 0.0


def calculate_retrieval_metrics(
    retrieved_docs: List[Dict],
    relevant_ids: set = None,
    relevance_map: Dict[str, float] = None,
    k: int = 5
) -> RetrievalMetrics:
    """ëª¨ë“  ê²€ìƒ‰ í’ˆì§ˆ ì§€í‘œ ê³„ì‚°"""
    relevant_ids = relevant_ids or set()
    relevance_map = relevance_map or {doc_id: 1.0 for doc_id in relevant_ids}

    return RetrievalMetrics(
        precision_at_k=precision_at_k(retrieved_docs, relevant_ids, k),
        recall_at_k=recall_at_k(retrieved_docs, relevant_ids, k),
        mrr=mean_reciprocal_rank(retrieved_docs, relevant_ids),
        ndcg_at_k=ndcg_at_k(retrieved_docs, relevance_map, k),
        hit_rate=hit_rate(retrieved_docs, relevant_ids, k),
    )


# ============================================================================
# 2. ì‹œë§¨í‹± ìœ ì‚¬ë„ ì§€í‘œ (Semantic Similarity Metrics)
# ============================================================================

@dataclass
class SemanticMetrics:
    """ì‹œë§¨í‹± ìœ ì‚¬ë„ ì§€í‘œ"""
    avg_similarity: float = 0.0          # í‰ê·  ì¿¼ë¦¬-ë¬¸ì„œ ìœ ì‚¬ë„
    max_similarity: float = 0.0          # ìµœëŒ€ ìœ ì‚¬ë„
    min_similarity: float = 0.0          # ìµœì†Œ ìœ ì‚¬ë„
    similarity_scores: List[float] = field(default_factory=list)


def get_embedding(text: str, model: str = "text-embedding-3-small") -> List[float]:
    """OpenAI ì„ë² ë”© ìƒì„±"""
    try:
        from app.rag.retriever.db import embed_query
        return embed_query(text, model)
    except Exception:
        client = get_openai_client()
        resp = client.embeddings.create(model=model, input=text)
        return resp.data[0].embedding


def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    a = np.array(vec1)
    b = np.array(vec2)
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))


def calculate_semantic_metrics(
    query: str,
    retrieved_docs: List[Dict],
    use_cached_embeddings: bool = True
) -> SemanticMetrics:
    """
    ì¿¼ë¦¬ì™€ ê²€ìƒ‰ëœ ë¬¸ì„œ ê°„ì˜ ì‹œë§¨í‹± ìœ ì‚¬ë„ ê³„ì‚°

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        retrieved_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        use_cached_embeddings: ë¬¸ì„œì— ì„ë² ë”©ì´ ìˆìœ¼ë©´ ì‚¬ìš©
    """
    if not retrieved_docs:
        return SemanticMetrics()

    query_embedding = get_embedding(query)
    similarity_scores = []

    for doc in retrieved_docs:
        # ë¬¸ì„œ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°
        doc_embedding = doc.get("embedding")
        if not doc_embedding or not use_cached_embeddings:
            content = doc.get("content", "") or doc.get("title", "")
            if not content:
                continue
            doc_embedding = get_embedding(content[:2000])  # í† í° ì œí•œ

        sim = cosine_similarity(query_embedding, doc_embedding)
        similarity_scores.append(sim)

    if not similarity_scores:
        return SemanticMetrics()

    return SemanticMetrics(
        avg_similarity=float(np.mean(similarity_scores)),
        max_similarity=float(np.max(similarity_scores)),
        min_similarity=float(np.min(similarity_scores)),
        similarity_scores=similarity_scores,
    )


# ============================================================================
# 3. RAGAS í”„ë ˆì„ì›Œí¬ ì§€í‘œ (LLM-as-Judge)
# ============================================================================

@dataclass
class RAGASMetrics:
    """RAGAS í”„ë ˆì„ì›Œí¬ í‰ê°€ ì§€í‘œ"""
    faithfulness: float = 0.0        # ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°í•˜ëŠ”ì§€
    answer_relevancy: float = 0.0    # ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆëŠ”ì§€
    context_precision: float = 0.0   # ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ì— ìœ ìš©í•œì§€
    context_recall: float = 0.0      # í•„ìš”í•œ ì •ë³´ê°€ ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ”ì§€
    overall_score: float = 0.0       # ì „ì²´ ì ìˆ˜


RAGAS_SYSTEM_PROMPT = """You are an expert evaluator for RAG (Retrieval-Augmented Generation) systems.
Evaluate the following based on the given criteria. Return ONLY a JSON object with numeric scores.
All scores must be between 0.0 and 1.0."""


def evaluate_faithfulness(answer: str, contexts: List[str]) -> float:
    """
    Faithfulness: ë‹µë³€ì´ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°í•˜ëŠ”ì§€ í‰ê°€

    - 1.0: ë‹µë³€ì˜ ëª¨ë“  ë‚´ìš©ì´ ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°í•¨
    - 0.0: ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì™€ ë¬´ê´€í•˜ê±°ë‚˜ í™˜ê°
    """
    if not answer or not contexts:
        return 0.0

    context_text = "\n---\n".join(contexts[:3])

    prompt = f"""Evaluate FAITHFULNESS: Does the answer contain ONLY information from the given contexts?

Context:
{context_text}

Answer:
{answer}

Score (0.0-1.0):
- 1.0 = All claims in answer are supported by context
- 0.5 = Some claims are supported, some are not
- 0.0 = Answer contains hallucinations or unsupported claims

Return JSON: {{"faithfulness": <score>}}"""

    try:
        client = get_openai_client()
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": RAGAS_SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            max_tokens=50,
        )
        import json
        result = json.loads(resp.choices[0].message.content)
        return float(result.get("faithfulness", 0.0))
    except Exception:
        return 0.0


def evaluate_answer_relevancy(question: str, answer: str) -> float:
    """
    Answer Relevancy: ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆëŠ”ì§€ í‰ê°€

    - 1.0: ë‹µë³€ì´ ì§ˆë¬¸ì„ ì™„ë²½íˆ í•´ê²°
    - 0.0: ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ë¬´ê´€
    """
    if not question or not answer:
        return 0.0

    prompt = f"""Evaluate ANSWER RELEVANCY: Does the answer address the question?

Question: {question}
Answer: {answer}

Score (0.0-1.0):
- 1.0 = Answer completely addresses the question
- 0.5 = Answer partially addresses the question
- 0.0 = Answer is irrelevant to the question

Return JSON: {{"answer_relevancy": <score>}}"""

    try:
        client = get_openai_client()
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": RAGAS_SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            max_tokens=50,
        )
        import json
        result = json.loads(resp.choices[0].message.content)
        return float(result.get("answer_relevancy", 0.0))
    except Exception:
        return 0.0


def evaluate_context_precision(question: str, contexts: List[str]) -> float:
    """
    Context Precision: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ í•´ê²°ì— ìœ ìš©í•œì§€

    - 1.0: ëª¨ë“  ì»¨í…ìŠ¤íŠ¸ê°€ ìœ ìš©
    - 0.0: ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ê³¼ ë¬´ê´€
    """
    if not question or not contexts:
        return 0.0

    context_text = "\n---\n".join(contexts[:3])

    prompt = f"""Evaluate CONTEXT PRECISION: Are the retrieved contexts useful for answering the question?

Question: {question}

Retrieved Contexts:
{context_text}

Score (0.0-1.0):
- 1.0 = All contexts are highly relevant and useful
- 0.5 = Some contexts are useful
- 0.0 = Contexts are irrelevant to the question

Return JSON: {{"context_precision": <score>}}"""

    try:
        client = get_openai_client()
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": RAGAS_SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            max_tokens=50,
        )
        import json
        result = json.loads(resp.choices[0].message.content)
        return float(result.get("context_precision", 0.0))
    except Exception:
        return 0.0


def evaluate_context_recall(question: str, contexts: List[str], ground_truth: str = None) -> float:
    """
    Context Recall: ì§ˆë¬¸ì— í•„ìš”í•œ ì •ë³´ê°€ ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ”ì§€

    ground_truthê°€ ì—†ìœ¼ë©´ ì§ˆë¬¸ ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡ 
    """
    if not question or not contexts:
        return 0.0

    context_text = "\n---\n".join(contexts[:3])

    if ground_truth:
        prompt = f"""Evaluate CONTEXT RECALL: Do the contexts contain all information needed for the ground truth answer?

Question: {question}
Ground Truth Answer: {ground_truth}

Retrieved Contexts:
{context_text}

Score (0.0-1.0):
- 1.0 = Contexts contain all information needed
- 0.5 = Contexts contain some needed information
- 0.0 = Contexts miss critical information

Return JSON: {{"context_recall": <score>}}"""
    else:
        prompt = f"""Evaluate CONTEXT RECALL: Do the contexts contain enough information to answer the question?

Question: {question}

Retrieved Contexts:
{context_text}

Score (0.0-1.0):
- 1.0 = Contexts have sufficient information
- 0.5 = Contexts have partial information
- 0.0 = Contexts lack necessary information

Return JSON: {{"context_recall": <score>}}"""

    try:
        client = get_openai_client()
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": RAGAS_SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            max_tokens=50,
        )
        import json
        result = json.loads(resp.choices[0].message.content)
        return float(result.get("context_recall", 0.0))
    except Exception:
        return 0.0


def calculate_ragas_metrics(
    question: str,
    answer: str = None,
    contexts: List[str] = None,
    ground_truth: str = None,
) -> RAGASMetrics:
    """RAGAS ì „ì²´ ì§€í‘œ ê³„ì‚°"""
    contexts = contexts or []

    faithfulness = evaluate_faithfulness(answer, contexts) if answer else 0.0
    answer_relevancy = evaluate_answer_relevancy(question, answer) if answer else 0.0
    context_precision = evaluate_context_precision(question, contexts)
    context_recall = evaluate_context_recall(question, contexts, ground_truth)

    # ì „ì²´ ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
    scores = [faithfulness, answer_relevancy, context_precision, context_recall]
    valid_scores = [s for s in scores if s > 0]
    overall = sum(valid_scores) / len(valid_scores) if valid_scores else 0.0

    return RAGASMetrics(
        faithfulness=faithfulness,
        answer_relevancy=answer_relevancy,
        context_precision=context_precision,
        context_recall=context_recall,
        overall_score=overall,
    )


# ============================================================================
# í†µí•© í‰ê°€ í´ë˜ìŠ¤
# ============================================================================

@dataclass
class RAGEvaluationResult:
    """ì „ì²´ RAG í‰ê°€ ê²°ê³¼"""
    query: str
    retrieval: RetrievalMetrics
    semantic: SemanticMetrics
    ragas: RAGASMetrics

    def to_dict(self) -> Dict[str, Any]:
        return {
            "query": self.query,
            "retrieval": {
                "precision_at_k": self.retrieval.precision_at_k,
                "recall_at_k": self.retrieval.recall_at_k,
                "mrr": self.retrieval.mrr,
                "ndcg_at_k": self.retrieval.ndcg_at_k,
                "hit_rate": self.retrieval.hit_rate,
            },
            "semantic": {
                "avg_similarity": self.semantic.avg_similarity,
                "max_similarity": self.semantic.max_similarity,
            },
            "ragas": {
                "faithfulness": self.ragas.faithfulness,
                "answer_relevancy": self.ragas.answer_relevancy,
                "context_precision": self.ragas.context_precision,
                "context_recall": self.ragas.context_recall,
                "overall_score": self.ragas.overall_score,
            },
        }


def evaluate_rag_response(
    query: str,
    retrieved_docs: List[Dict],
    answer: str = None,
    relevant_ids: set = None,
    ground_truth: str = None,
    k: int = 5,
    evaluate_semantic: bool = False,  # ë¹„ìš© ì ˆê°ì„ ìœ„í•´ ê¸°ë³¸ ë¹„í™œì„±í™”
    evaluate_ragas: bool = True,
) -> RAGEvaluationResult:
    """
    RAG ì‘ë‹µ ì¢…í•© í‰ê°€

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        retrieved_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        answer: ìƒì„±ëœ ë‹µë³€ (ì„ íƒ)
        relevant_ids: ê´€ë ¨ ë¬¸ì„œ ID set (Ground Truth)
        ground_truth: ì •ë‹µ í…ìŠ¤íŠ¸ (ì„ íƒ)
        k: Top-K
        evaluate_semantic: ì‹œë§¨í‹± ìœ ì‚¬ë„ í‰ê°€ ì—¬ë¶€
        evaluate_ragas: RAGAS í‰ê°€ ì—¬ë¶€
    """
    # 1. ê²€ìƒ‰ í’ˆì§ˆ ì§€í‘œ
    retrieval = calculate_retrieval_metrics(
        retrieved_docs, relevant_ids, k=k
    )

    # 2. ì‹œë§¨í‹± ìœ ì‚¬ë„ (ì„ íƒ)
    if evaluate_semantic:
        semantic = calculate_semantic_metrics(query, retrieved_docs)
    else:
        semantic = SemanticMetrics()

    # 3. RAGAS í‰ê°€ (ì„ íƒ)
    if evaluate_ragas:
        contexts = [doc.get("content", "") for doc in retrieved_docs[:3]]
        ragas = calculate_ragas_metrics(
            question=query,
            answer=answer,
            contexts=contexts,
            ground_truth=ground_truth,
        )
    else:
        ragas = RAGASMetrics()

    return RAGEvaluationResult(
        query=query,
        retrieval=retrieval,
        semantic=semantic,
        ragas=ragas,
    )


# ============================================================================
# ë°°ì¹˜ í‰ê°€ ìœ í‹¸ë¦¬í‹°
# ============================================================================

def aggregate_metrics(results: List[RAGEvaluationResult]) -> Dict[str, float]:
    """ì—¬ëŸ¬ í‰ê°€ ê²°ê³¼ì˜ í‰ê·  ê³„ì‚°"""
    if not results:
        return {}

    return {
        # Retrieval
        "avg_precision_at_k": np.mean([r.retrieval.precision_at_k for r in results]),
        "avg_recall_at_k": np.mean([r.retrieval.recall_at_k for r in results]),
        "avg_mrr": np.mean([r.retrieval.mrr for r in results]),
        "avg_ndcg_at_k": np.mean([r.retrieval.ndcg_at_k for r in results]),
        "avg_hit_rate": np.mean([r.retrieval.hit_rate for r in results]),
        # Semantic
        "avg_semantic_similarity": np.mean([r.semantic.avg_similarity for r in results if r.semantic.avg_similarity > 0]),
        # RAGAS
        "avg_faithfulness": np.mean([r.ragas.faithfulness for r in results if r.ragas.faithfulness > 0]),
        "avg_answer_relevancy": np.mean([r.ragas.answer_relevancy for r in results if r.ragas.answer_relevancy > 0]),
        "avg_context_precision": np.mean([r.ragas.context_precision for r in results if r.ragas.context_precision > 0]),
        "avg_context_recall": np.mean([r.ragas.context_recall for r in results if r.ragas.context_recall > 0]),
        "avg_ragas_overall": np.mean([r.ragas.overall_score for r in results if r.ragas.overall_score > 0]),
    }


def print_evaluation_summary(results: List[RAGEvaluationResult]):
    """í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    agg = aggregate_metrics(results)

    print("\n" + "=" * 70)
    print("RAG í‰ê°€ ì§€í‘œ ìš”ì•½")
    print("=" * 70)

    print("\nğŸ“Š ê²€ìƒ‰ í’ˆì§ˆ ì§€í‘œ (Retrieval Quality)")
    print(f"  Precision@K:    {agg.get('avg_precision_at_k', 0):.3f}")
    print(f"  Recall@K:       {agg.get('avg_recall_at_k', 0):.3f}")
    print(f"  MRR:            {agg.get('avg_mrr', 0):.3f}")
    print(f"  NDCG@K:         {agg.get('avg_ndcg_at_k', 0):.3f}")
    print(f"  Hit Rate:       {agg.get('avg_hit_rate', 0):.3f}")

    if agg.get('avg_semantic_similarity', 0) > 0:
        print("\nğŸ“Š ì‹œë§¨í‹± ìœ ì‚¬ë„ (Semantic Similarity)")
        print(f"  í‰ê·  ìœ ì‚¬ë„:     {agg.get('avg_semantic_similarity', 0):.3f}")

    if agg.get('avg_ragas_overall', 0) > 0:
        print("\nğŸ“Š RAGAS ì§€í‘œ (LLM-as-Judge)")
        print(f"  Faithfulness:       {agg.get('avg_faithfulness', 0):.3f}")
        print(f"  Answer Relevancy:   {agg.get('avg_answer_relevancy', 0):.3f}")
        print(f"  Context Precision:  {agg.get('avg_context_precision', 0):.3f}")
        print(f"  Context Recall:     {agg.get('avg_context_recall', 0):.3f}")
        print(f"  Overall Score:      {agg.get('avg_ragas_overall', 0):.3f}")

    print("=" * 70)
