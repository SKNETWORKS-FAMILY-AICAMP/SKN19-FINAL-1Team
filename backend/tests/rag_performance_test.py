"""
RAG ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

í…ŒìŠ¤íŠ¸ í•­ëª©:
1. Retrieval í’ˆì§ˆ í…ŒìŠ¤íŠ¸ (Precision, Recall, MRR)
2. ì‘ë‹µ ì†ë„ í…ŒìŠ¤íŠ¸ (ì§€ì—° ì‹œê°„)
3. ì¿¼ë¦¬ ë¼ìš°íŒ… ì •í™•ë„ í…ŒìŠ¤íŠ¸
"""

import asyncio
import time
import sys
import os
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Tuple
from collections import defaultdict
import statistics

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dotenv import load_dotenv
load_dotenv()

from app.rag.pipeline.search import run_search, route
from app.rag.router.router import route_query
from app.rag.retriever.db import warmup_embed_cache

# RAG í‰ê°€ ì§€í‘œ ëª¨ë“ˆ
from tests.rag_metrics import (
    RAGEvaluationResult,
    evaluate_rag_response,
    aggregate_metrics,
    print_evaluation_summary,
)

# Ground Truth í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ (test_suite.py)
from app.rag.tests.test_suite import TESTS as SUITE_TESTS


# ============================================================================
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì •ì˜
# ============================================================================

@dataclass
class TestCase:
    """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤"""
    query: str                              # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    expected_route: str                     # ê¸°ëŒ€ ë¼ìš°íŒ… (card_info, card_usage, none)
    expected_keywords: List[str] = field(default_factory=list)  # ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ë˜ì–´ì•¼ í•  í‚¤ì›Œë“œ
    category: str = ""                      # í…ŒìŠ¤íŠ¸ ì¹´í…Œê³ ë¦¬
    # Ground Truth (test_suite.py í˜•ì‹)
    must_have_doc_ids: List[str] = field(default_factory=list)  # ë°˜ë“œì‹œ ê²€ìƒ‰ë˜ì–´ì•¼ í•  ë¬¸ì„œ ID
    must_not_have_doc_ids: List[str] = field(default_factory=list)  # ê²€ìƒ‰ë˜ë©´ ì•ˆë˜ëŠ” ë¬¸ì„œ ID
    must_have_answer_terms: List[str] = field(default_factory=list)  # ë‹µë³€ì— í¬í•¨ë˜ì–´ì•¼ í•  í‚¤ì›Œë“œ
    must_not_have_answer_terms: List[str] = field(default_factory=list)  # ë‹µë³€ì— í¬í•¨ë˜ë©´ ì•ˆë˜ëŠ” í‚¤ì›Œë“œ
    test_id: str = ""  # í…ŒìŠ¤íŠ¸ ID


# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜ (DB ì¹´í…Œê³ ë¦¬ ê¸°ì¤€: 8ëŒ€ë¶„ë¥˜)
# ëŒ€ë¶„ë¥˜: ê²°ì œ/ìŠ¹ì¸, ì´ìš©ë‚´ì—­, í•œë„, ë¶„ì‹¤/ë„ë‚œ, ìˆ˜ìˆ˜ë£Œ/ì—°ì²´, í¬ì¸íŠ¸/í˜œíƒ, ì •ë¶€ì§€ì›, ê¸°íƒ€
TEST_CASES = [
    # ===== ì¹´ë“œ ì •ë³´ ì¡°íšŒ (card_info) =====
    # í¬ì¸íŠ¸/í˜œíƒ
    TestCase("ë‚˜ë¼ì‚¬ë‘ì¹´ë“œ í˜œíƒ ì•Œë ¤ì£¼ì„¸ìš”", "card_info", ["ë‚˜ë¼ì‚¬ë‘ì¹´ë“œ", "í˜œíƒ"], "í¬ì¸íŠ¸/í˜œíƒ"),
    TestCase("K-íŒ¨ìŠ¤ í• ì¸ ë˜ëŠ” ì¹´ë“œ ì¶”ì²œí•´ì£¼ì„¸ìš”", "card_info", ["K-íŒ¨ìŠ¤", "í• ì¸"], "í¬ì¸íŠ¸/í˜œíƒ"),
    TestCase("ì£¼ìœ  í• ì¸ ì¹´ë“œ ìˆì–´ìš”?", "card_info", ["ì£¼ìœ ", "í• ì¸"], "í¬ì¸íŠ¸/í˜œíƒ"),
    TestCase("ë§ˆì¼ë¦¬ì§€ ì ë¦½ ì˜ ë˜ëŠ” ì¹´ë“œ ì¶”ì²œí•´ì£¼ì„¸ìš”", "card_info", ["ë§ˆì¼ë¦¬ì§€", "ì ë¦½"], "í¬ì¸íŠ¸/í˜œíƒ"),

    # ê¸°íƒ€ (ì¹´ë“œ ë°œê¸‰/ì •ë³´)
    TestCase("êµ­ë¯¼í–‰ë³µì¹´ë“œ ë°œê¸‰ ì¡°ê±´ì´ ë­ì˜ˆìš”?", "card_info", ["êµ­ë¯¼í–‰ë³µì¹´ë“œ", "ë°œê¸‰"], "ê¸°íƒ€"),
    TestCase("ì‹ ìš©ì¹´ë“œ ë°œê¸‰í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ìš”?", "card_info", ["ì‹ ìš©ì¹´ë“œ", "ë°œê¸‰"], "ê¸°íƒ€"),

    # ===== ì¹´ë“œ ì‚¬ìš©ë²•/ì„œë¹„ìŠ¤ ì•ˆë‚´ (card_usage) =====
    # ë¶„ì‹¤/ë„ë‚œ
    TestCase("ì¹´ë“œ ë¶„ì‹¤ì‹ ê³  í•˜ë ¤ê³ ìš”", "card_usage", ["ë¶„ì‹¤"], "ë¶„ì‹¤/ë„ë‚œ"),
    TestCase("ì¹´ë“œë¥¼ ìƒì–´ë²„ë ¸ì–´ìš”", "card_usage", ["ë¶„ì‹¤", "ë„ë‚œ"], "ë¶„ì‹¤/ë„ë‚œ"),
    TestCase("ì¹´ë“œ ë„ë‚œë‹¹í–ˆì–´ìš” ì–´ë–»ê²Œ í•´ìš”?", "card_usage", ["ë„ë‚œ"], "ë¶„ì‹¤/ë„ë‚œ"),

    # ê²°ì œ/ìŠ¹ì¸
    TestCase("ê²°ì œê°€ ì•ˆë¼ìš”", "card_usage", ["ê²°ì œ"], "ê²°ì œ/ìŠ¹ì¸"),
    TestCase("ê²°ì œì¼ ë³€ê²½í•˜ê³  ì‹¶ì–´ìš”", "card_usage", ["ê²°ì œì¼", "ë³€ê²½"], "ê²°ì œ/ìŠ¹ì¸"),
    TestCase("ì´ë²ˆ ë‹¬ ê²°ì œê¸ˆì•¡ ì•Œë ¤ì£¼ì„¸ìš”", "card_usage", ["ê²°ì œ", "ê¸ˆì•¡"], "ê²°ì œ/ìŠ¹ì¸"),

    # í•œë„
    TestCase("ì¹´ë“œ í•œë„ ì˜¬ë¦¬ê³  ì‹¶ì–´ìš”", "card_usage", ["í•œë„"], "í•œë„"),
    TestCase("ì´ìš©í•œë„ ì¡°íšŒí•˜ë ¤ê³ ìš”", "card_usage", ["í•œë„", "ì¡°íšŒ"], "í•œë„"),

    # ìˆ˜ìˆ˜ë£Œ/ì—°ì²´
    TestCase("ë¦¬ë³¼ë¹™ ì‹ ì²­í•˜ê³  ì‹¶ì–´ìš”", "card_usage", ["ë¦¬ë³¼ë¹™", "ì‹ ì²­"], "ìˆ˜ìˆ˜ë£Œ/ì—°ì²´"),
    TestCase("ë¦¬ë³¼ë¹™ ì·¨ì†Œ ë°©ë²•", "card_usage", ["ë¦¬ë³¼ë¹™", "ì·¨ì†Œ"], "ìˆ˜ìˆ˜ë£Œ/ì—°ì²´"),
    TestCase("ì—°ì²´ì´ì ì–¼ë§ˆì˜ˆìš”?", "card_usage", ["ì—°ì²´", "ì´ì"], "ìˆ˜ìˆ˜ë£Œ/ì—°ì²´"),

    # ê¸°íƒ€ (ì„œë¹„ìŠ¤)
    TestCase("ì¹´ë“œë¡  ì‹ ì²­í•˜ë ¤ê³ ìš”", "card_usage", ["ì¹´ë“œë¡ ", "ì‹ ì²­"], "ê¸°íƒ€"),
    TestCase("í˜„ê¸ˆì„œë¹„ìŠ¤ ì´ìš©ë°©ë²•", "card_usage", ["í˜„ê¸ˆì„œë¹„ìŠ¤"], "ê¸°íƒ€"),
    TestCase("ì‚¼ì„±í˜ì´ ë“±ë¡í•˜ë ¤ê³ ìš”", "card_usage", ["ì‚¼ì„±í˜ì´", "ë“±ë¡"], "ê¸°íƒ€"),
    TestCase("ì¹´ë“œ í•´ì§€í•˜ë ¤ê³ ìš”", "card_usage", ["í•´ì§€"], "ê¸°íƒ€"),
    TestCase("ì¹´ë“œ ì¬ë°œê¸‰ ì‹ ì²­í•˜ë ¤ê³ ìš”", "card_usage", ["ì¬ë°œê¸‰"], "ê¸°íƒ€"),

    # ì •ë¶€ì§€ì›
    TestCase("êµ­ë¯¼í–‰ë³µì¹´ë“œ ë°”ìš°ì²˜ ì‚¬ìš©ë²•", "card_usage", ["êµ­ë¯¼í–‰ë³µì¹´ë“œ", "ë°”ìš°ì²˜"], "ì •ë¶€ì§€ì›"),

    # ì´ìš©ë‚´ì—­
    TestCase("ì´ë²ˆ ë‹¬ ì¹´ë“œ ì‚¬ìš©ë‚´ì—­ ë³´ì—¬ì£¼ì„¸ìš”", "card_usage", ["ì‚¬ìš©ë‚´ì—­", "ì´ìš©ë‚´ì—­"], "ì´ìš©ë‚´ì—­"),

    # ë³µí•© ì¿¼ë¦¬
    TestCase("ë‚˜ë¼ì‚¬ë‘ì¹´ë“œ ë¶„ì‹¤í•´ì„œ ì¬ë°œê¸‰ ë°›ê³  ì‹¶ì–´ìš”", "card_usage", ["ë‚˜ë¼ì‚¬ë‘ì¹´ë“œ", "ë¶„ì‹¤", "ë°œê¸‰"], "ë¶„ì‹¤/ë„ë‚œ"),
    TestCase("ë¦¬ë³¼ë¹™ ì„œë¹„ìŠ¤ ì‹ ì²­í–ˆëŠ”ë° ì·¨ì†Œí•˜ê³  ì‹¶ì–´ìš”", "card_usage", ["ë¦¬ë³¼ë¹™", "ì·¨ì†Œ"], "ìˆ˜ìˆ˜ë£Œ/ì—°ì²´"),
]


def _suite_to_testcase(t: Dict[str, Any]) -> TestCase:
    """test_suite.py í˜•ì‹ì„ TestCaseë¡œ ë³€í™˜"""
    route = t.get("expect_route", "card_usage")
    # ì¹´í…Œê³ ë¦¬ ì¶”ì •
    query = t.get("query", "").lower()
    if "ë¶„ì‹¤" in query or "ë„ë‚œ" in query or "ìƒì–´ë²„" in query:
        category = "ë¶„ì‹¤/ë„ë‚œ"
    elif "ê²°ì œ" in query or "ìŠ¹ì¸" in query:
        category = "ê²°ì œ/ìŠ¹ì¸"
    elif "í•œë„" in query:
        category = "í•œë„"
    elif "ë¦¬ë³¼ë¹™" in query or "ì—°ì²´" in query or "ì´ì" in query or "ìˆ˜ìˆ˜ë£Œ" in query:
        category = "ìˆ˜ìˆ˜ë£Œ/ì—°ì²´"
    elif "í˜œíƒ" in query or "í¬ì¸íŠ¸" in query or "ì ë¦½" in query or "í• ì¸" in query:
        category = "í¬ì¸íŠ¸/í˜œíƒ"
    elif "êµ­ë¯¼í–‰ë³µ" in query or "ë°”ìš°ì²˜" in query or "ë‹¤ë‘¥ì´" in query:
        category = "ì •ë¶€ì§€ì›"
    elif "kíŒ¨ìŠ¤" in query or "k-íŒ¨ìŠ¤" in query:
        category = "í¬ì¸íŠ¸/í˜œíƒ"
    else:
        category = "ê¸°íƒ€"

    return TestCase(
        query=t.get("query", ""),
        expected_route=route,
        expected_keywords=t.get("must_have_answer_terms", []),
        category=category,
        must_have_doc_ids=t.get("must_have_doc_ids", []),
        must_not_have_doc_ids=t.get("must_not_have_doc_ids", []),
        must_have_answer_terms=t.get("must_have_answer_terms", []),
        must_not_have_answer_terms=t.get("must_not_have_answer_terms", []),
        test_id=t.get("id", ""),
    )


# Ground Truth í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ (test_suite.py ê¸°ë°˜)
GROUND_TRUTH_CASES = [_suite_to_testcase(t) for t in SUITE_TESTS]


# ============================================================================
# ì„±ëŠ¥ ì¸¡ì • í´ë˜ìŠ¤
# ============================================================================

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼"""
    # ì†ë„ ê´€ë ¨
    latencies: List[float] = field(default_factory=list)
    route_times: List[float] = field(default_factory=list)
    retrieve_times: List[float] = field(default_factory=list)

    # í’ˆì§ˆ ê´€ë ¨
    routing_correct: int = 0
    routing_total: int = 0
    keyword_hits: int = 0
    keyword_total: int = 0
    docs_returned: List[int] = field(default_factory=list)

    # ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼
    category_results: Dict[str, Dict[str, Any]] = field(default_factory=lambda: defaultdict(lambda: {
        "correct": 0, "total": 0, "latencies": []
    }))

    # RAG í‰ê°€ ì§€í‘œ
    rag_evaluations: List[RAGEvaluationResult] = field(default_factory=list)

    def add_result(
        self,
        latency: float,
        route_time: float,
        retrieve_time: float,
        routing_correct: bool,
        keyword_hit_count: int,
        keyword_total_count: int,
        docs_count: int,
        category: str
    ):
        self.latencies.append(latency)
        self.route_times.append(route_time)
        self.retrieve_times.append(retrieve_time)

        if routing_correct:
            self.routing_correct += 1
        self.routing_total += 1

        self.keyword_hits += keyword_hit_count
        self.keyword_total += keyword_total_count
        self.docs_returned.append(docs_count)

        # ì¹´í…Œê³ ë¦¬ë³„ ì €ì¥
        self.category_results[category]["total"] += 1
        if routing_correct:
            self.category_results[category]["correct"] += 1
        self.category_results[category]["latencies"].append(latency)

    def summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ë°˜í™˜"""
        return {
            "latency": {
                "mean_ms": statistics.mean(self.latencies) * 1000 if self.latencies else 0,
                "median_ms": statistics.median(self.latencies) * 1000 if self.latencies else 0,
                "p95_ms": self._percentile(self.latencies, 95) * 1000 if self.latencies else 0,
                "p99_ms": self._percentile(self.latencies, 99) * 1000 if self.latencies else 0,
                "min_ms": min(self.latencies) * 1000 if self.latencies else 0,
                "max_ms": max(self.latencies) * 1000 if self.latencies else 0,
            },
            "route_time": {
                "mean_ms": statistics.mean(self.route_times) * 1000 if self.route_times else 0,
            },
            "retrieve_time": {
                "mean_ms": statistics.mean(self.retrieve_times) * 1000 if self.retrieve_times else 0,
            },
            "routing_accuracy": self.routing_correct / self.routing_total if self.routing_total else 0,
            "keyword_precision": self.keyword_hits / self.keyword_total if self.keyword_total else 0,
            "avg_docs_returned": statistics.mean(self.docs_returned) if self.docs_returned else 0,
            "total_tests": self.routing_total,
        }

    def _percentile(self, data: List[float], p: int) -> float:
        if not data:
            return 0
        sorted_data = sorted(data)
        k = (len(sorted_data) - 1) * p / 100
        f = int(k)
        c = f + 1
        if c >= len(sorted_data):
            return sorted_data[-1]
        return sorted_data[f] + (sorted_data[c] - sorted_data[f]) * (k - f)


# ============================================================================
# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
# ============================================================================

async def test_single_query(
    test_case: TestCase,
    top_k: int = 5,
    verbose: bool = False,
    evaluate_ragas: bool = False,
) -> Tuple[Dict[str, Any], PerformanceMetrics]:
    """ë‹¨ì¼ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸"""
    metrics = PerformanceMetrics()

    start_time = time.perf_counter()

    try:
        result = await run_search(
            query=test_case.query,
            top_k=top_k,
            enable_consult_search=False
        )

        end_time = time.perf_counter()
        latency = end_time - start_time
        route_time = result.t_route - result.t_start
        retrieve_time = result.t_retrieve - result.t_route

        # ë¼ìš°íŒ… ì •í™•ë„ ì²´í¬
        actual_route = result.routing.get("route") or result.routing.get("ui_route")
        routing_correct = actual_route == test_case.expected_route

        # í‚¤ì›Œë“œ ë§¤ì¹­ ì²´í¬
        keyword_hits = 0
        if test_case.expected_keywords and result.docs:
            all_content = " ".join([
                str(doc.get("title", "")) + " " + str(doc.get("content", ""))
                for doc in result.docs
            ]).lower()

            for keyword in test_case.expected_keywords:
                if keyword.lower() in all_content:
                    keyword_hits += 1

        metrics.add_result(
            latency=latency,
            route_time=route_time,
            retrieve_time=retrieve_time,
            routing_correct=routing_correct,
            keyword_hit_count=keyword_hits,
            keyword_total_count=len(test_case.expected_keywords),
            docs_count=len(result.docs),
            category=test_case.category
        )

        # RAG í‰ê°€ ì§€í‘œ ê³„ì‚° (ì„ íƒì )
        if evaluate_ragas and result.docs:
            # Ground Truth ë¬¸ì„œ IDê°€ ìˆìœ¼ë©´ retrieval metrics ê³„ì‚° ê°€ëŠ¥
            relevant_ids = set(test_case.must_have_doc_ids) if test_case.must_have_doc_ids else None
            rag_eval = evaluate_rag_response(
                query=test_case.query,
                retrieved_docs=result.docs,
                answer=None,  # ë‹µë³€ ìƒì„±ì´ ì—†ìœ¼ë¯€ë¡œ None
                relevant_ids=relevant_ids,
                evaluate_semantic=False,
                evaluate_ragas=True,
            )
            metrics.rag_evaluations.append(rag_eval)

        detail = {
            "query": test_case.query,
            "expected_route": test_case.expected_route,
            "actual_route": actual_route,
            "routing_correct": routing_correct,
            "latency_ms": latency * 1000,
            "docs_count": len(result.docs),
            "keyword_hits": keyword_hits,
            "keyword_total": len(test_case.expected_keywords),
            "retrieval_mode": result.routing.get("retrieval_mode"),
            "cache_status": result.retrieve_cache_status,
        }

        if verbose and result.docs:
            detail["top_doc_title"] = result.docs[0].get("title", "N/A")[:50]
            detail["top_doc_score"] = result.docs[0].get("score", 0)

        return detail, metrics

    except Exception as e:
        return {"query": test_case.query, "error": str(e)}, metrics


async def run_performance_test(
    test_cases: List[TestCase] = None,
    top_k: int = 5,
    iterations: int = 1,
    verbose: bool = True,
    evaluate_ragas: bool = False,
) -> Dict[str, Any]:
    """ì „ì²´ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""

    if test_cases is None:
        test_cases = TEST_CASES

    # ì„ë² ë”© ìºì‹œ ì›Œë°ì—… (ì‹¤ì œ ê²€ìƒ‰ ì‹¤í–‰ìœ¼ë¡œ ìºì‹œ ì±„ìš°ê¸°)
    print("=" * 70)
    print("ìºì‹œ ì›Œë°ì—… ì¤‘...")
    for tc in test_cases:
        try:
            await run_search(tc.query, top_k=top_k)
        except Exception:
            pass
    print("ì›Œë°ì—… ì™„ë£Œ")

    print("=" * 70)
    print("RAG ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    print(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìˆ˜: {len(test_cases)}")
    print(f"ë°˜ë³µ íšŸìˆ˜: {iterations}")
    print(f"Top-K: {top_k}")
    print(f"RAGAS í‰ê°€: {'ON' if evaluate_ragas else 'OFF'}")
    print("=" * 70)

    all_metrics = PerformanceMetrics()
    all_details = []

    for iteration in range(iterations):
        if iterations > 1:
            print(f"\n[ë°˜ë³µ {iteration + 1}/{iterations}]")

        for i, test_case in enumerate(test_cases, 1):
            detail, metrics = await test_single_query(
                test_case=test_case,
                top_k=top_k,
                verbose=verbose,
                evaluate_ragas=evaluate_ragas,
            )

            # ë©”íŠ¸ë¦­ ë³‘í•©
            all_metrics.latencies.extend(metrics.latencies)
            all_metrics.route_times.extend(metrics.route_times)
            all_metrics.retrieve_times.extend(metrics.retrieve_times)
            all_metrics.routing_correct += metrics.routing_correct
            all_metrics.routing_total += metrics.routing_total
            all_metrics.keyword_hits += metrics.keyword_hits
            all_metrics.keyword_total += metrics.keyword_total
            all_metrics.docs_returned.extend(metrics.docs_returned)
            all_metrics.rag_evaluations.extend(metrics.rag_evaluations)

            for cat, cat_data in metrics.category_results.items():
                all_metrics.category_results[cat]["total"] += cat_data["total"]
                all_metrics.category_results[cat]["correct"] += cat_data["correct"]
                all_metrics.category_results[cat]["latencies"].extend(cat_data["latencies"])

            all_details.append(detail)

            # ì§„í–‰ ìƒí™© ì¶œë ¥
            status = "âœ“" if detail.get("routing_correct", False) else "âœ—"
            latency = detail.get("latency_ms", 0)
            print(f"  [{i}/{len(test_cases)}] {status} {test_case.query[:40]:<40} | {latency:>7.1f}ms | docs: {detail.get('docs_count', 0)}")

    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    summary = all_metrics.summary()

    print("\n" + "=" * 70)
    print("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 70)

    print("\nğŸ“Š ì†ë„ ì„±ëŠ¥")
    print(f"  í‰ê·  ì‘ë‹µ ì‹œê°„:    {summary['latency']['mean_ms']:.1f}ms")
    print(f"  ì¤‘ì•™ê°’ ì‘ë‹µ ì‹œê°„:  {summary['latency']['median_ms']:.1f}ms")
    print(f"  P95 ì‘ë‹µ ì‹œê°„:     {summary['latency']['p95_ms']:.1f}ms")
    print(f"  P99 ì‘ë‹µ ì‹œê°„:     {summary['latency']['p99_ms']:.1f}ms")
    print(f"  ìµœì†Œ/ìµœëŒ€:         {summary['latency']['min_ms']:.1f}ms / {summary['latency']['max_ms']:.1f}ms")

    print("\nğŸ“Š ë‹¨ê³„ë³„ ì†Œìš” ì‹œê°„")
    print(f"  ë¼ìš°íŒ… í‰ê· :       {summary['route_time']['mean_ms']:.1f}ms")
    print(f"  ê²€ìƒ‰ í‰ê· :         {summary['retrieve_time']['mean_ms']:.1f}ms")

    print("\nğŸ“Š í’ˆì§ˆ ì§€í‘œ")
    print(f"  ë¼ìš°íŒ… ì •í™•ë„:     {summary['routing_accuracy']*100:.1f}% ({all_metrics.routing_correct}/{all_metrics.routing_total})")
    print(f"  í‚¤ì›Œë“œ ì ì¤‘ë¥ :     {summary['keyword_precision']*100:.1f}% ({all_metrics.keyword_hits}/{all_metrics.keyword_total})")
    print(f"  í‰ê·  ë¬¸ì„œ ìˆ˜:      {summary['avg_docs_returned']:.1f}ê°œ")

    print("\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼")
    for cat, cat_data in sorted(all_metrics.category_results.items()):
        acc = cat_data["correct"] / cat_data["total"] * 100 if cat_data["total"] else 0
        avg_lat = statistics.mean(cat_data["latencies"]) * 1000 if cat_data["latencies"] else 0
        print(f"  {cat:<20} ì •í™•ë„: {acc:>5.1f}% | í‰ê· : {avg_lat:>6.1f}ms")

    # RAGAS í‰ê°€ ê²°ê³¼ ì¶œë ¥
    if all_metrics.rag_evaluations:
        print_evaluation_summary(all_metrics.rag_evaluations)

    print("\n" + "=" * 70)

    return {
        "summary": summary,
        "details": all_details,
        "category_results": dict(all_metrics.category_results),
        "rag_evaluations": [e.to_dict() for e in all_metrics.rag_evaluations] if all_metrics.rag_evaluations else [],
    }


async def test_routing_only(test_cases: List[TestCase] = None) -> Dict[str, Any]:
    """ë¼ìš°íŒ…ë§Œ í…ŒìŠ¤íŠ¸ (DB ì—°ê²° ì—†ì´)"""

    if test_cases is None:
        test_cases = TEST_CASES

    print("=" * 70)
    print("ë¼ìš°íŒ… ì •í™•ë„ í…ŒìŠ¤íŠ¸ (DB ì—°ê²° ë¶ˆí•„ìš”)")
    print("=" * 70)

    correct = 0
    total = len(test_cases)
    results = []

    for test_case in test_cases:
        start = time.perf_counter()
        routing = route_query(test_case.query)
        elapsed = time.perf_counter() - start

        actual_route = routing.get("route") or routing.get("ui_route")
        is_correct = actual_route == test_case.expected_route

        if is_correct:
            correct += 1

        status = "âœ“" if is_correct else "âœ—"
        print(f"  {status} {test_case.query[:50]:<50} | ì˜ˆìƒ: {test_case.expected_route:<12} | ì‹¤ì œ: {actual_route:<12} | {elapsed*1000:.1f}ms")

        results.append({
            "query": test_case.query,
            "expected": test_case.expected_route,
            "actual": actual_route,
            "correct": is_correct,
            "latency_ms": elapsed * 1000,
            "routing_details": routing
        })

    print("\n" + "=" * 70)
    print(f"ë¼ìš°íŒ… ì •í™•ë„: {correct}/{total} ({correct/total*100:.1f}%)")
    print("=" * 70)

    return {
        "accuracy": correct / total,
        "correct": correct,
        "total": total,
        "results": results
    }


# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================

async def main():
    import argparse

    parser = argparse.ArgumentParser(description="RAG ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    parser.add_argument("--mode", choices=["full", "routing", "speed", "ground-truth"], default="full",
                       help="í…ŒìŠ¤íŠ¸ ëª¨ë“œ: full(ì „ì²´), routing(ë¼ìš°íŒ…ë§Œ), speed(ì†ë„ë§Œ), ground-truth(test_suite.py ê¸°ë°˜)")
    parser.add_argument("--iterations", type=int, default=1, help="ë°˜ë³µ íšŸìˆ˜")
    parser.add_argument("--top-k", type=int, default=5, help="ê²€ìƒ‰ ê²°ê³¼ ìˆ˜")
    parser.add_argument("--verbose", action="store_true", help="ìƒì„¸ ì¶œë ¥")
    parser.add_argument("--ragas", action="store_true", help="RAGAS í‰ê°€ ì§€í‘œ í™œì„±í™” (LLM í˜¸ì¶œ)")

    args = parser.parse_args()

    if args.mode == "routing":
        await test_routing_only()
    elif args.mode == "speed":
        # ë™ì¼ ì¿¼ë¦¬ ë°˜ë³µìœ¼ë¡œ ì†ë„ë§Œ ì¸¡ì •
        speed_cases = [
            TestCase("ì¹´ë“œ ë¶„ì‹¤ì‹ ê³ ", "card_usage"),
            TestCase("ë‚˜ë¼ì‚¬ë‘ì¹´ë“œ í˜œíƒ", "card_info"),
        ]
        await run_performance_test(
            test_cases=speed_cases,
            top_k=args.top_k,
            iterations=args.iterations * 5,
            verbose=args.verbose,
            evaluate_ragas=False,
        )
    elif args.mode == "ground-truth":
        # test_suite.py ê¸°ë°˜ Ground Truth í…ŒìŠ¤íŠ¸ (retrieval metrics ê³„ì‚°)
        await run_performance_test(
            test_cases=GROUND_TRUTH_CASES,
            top_k=args.top_k,
            iterations=args.iterations,
            verbose=args.verbose,
            evaluate_ragas=True,  # Ground Truthê°€ ìˆìœ¼ë¯€ë¡œ RAGAS í™œì„±í™”
        )
    else:
        await run_performance_test(
            top_k=args.top_k,
            iterations=args.iterations,
            verbose=args.verbose,
            evaluate_ragas=args.ragas,
        )


if __name__ == "__main__":
    asyncio.run(main())
