"""
Persona Generator í…ŒìŠ¤íŠ¸
"""
import json
import os
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€ (ëª¨ë“ˆ import ë¬¸ì œ í•´ê²°)
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))

from app.llm.education.persona_generator import create_system_prompt
from app.llm.education.client import generate_text, SIM_RUNPOD_URL

def test_model_connection():
    """ëª¨ë¸ ì—°ê²° í™•ì¸"""
    print("=" * 60)
    print("RunPod ëª¨ë¸ ì—°ê²° í™•ì¸")
    print("=" * 60)
    
    if not SIM_RUNPOD_URL:
        print("âŒ SIM_RUNPOD_URL í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì„ í™•ì¸í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return False
        
    print(f"URL: {SIM_RUNPOD_URL}")
    
    try:
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­
        print("ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...", end=" ", flush=True)
        response = generate_text("í…ŒìŠ¤íŠ¸ ì—°ê²°", max_tokens=10)
        
        if response:
             print("âœ… ì„±ê³µ")
             print(f"ì‘ë‹µ ìƒ˜í”Œ: {response}")
             return True
        else:
             print("âŒ ì‹¤íŒ¨ (ì‘ë‹µ ì—†ìŒ)")
             return False
    except Exception as e:
        print(f"\nâŒ ì—°ê²° ì‹¤íŒ¨: {e}")
        return False

def test_persona_generation_with_llm():
    """í•˜ë“œì½”ë”©ëœ ê³ ê° ë°ì´í„°ë¡œ í˜ë¥´ì†Œë‚˜ ìƒì„± ë° LLM ì‘ë‹µ í…ŒìŠ¤íŠ¸"""
    
    print("\n" + "=" * 60)
    print("Persona Generator & LLM ì—°ë™ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # 1. í•˜ë“œì½”ë”©ëœ ê³ ê° ì •ë³´
    # customer.json í˜•ì‹ì„ ì°¸ì¡°í•˜ì—¬ í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ìƒì„±
    customer_profile = {
        "name": "ë°•ì² ìˆ˜",
        "age_group": "50ëŒ€",
        "grade": "VIP",
        "gender": "male",
        # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì…ë ¥ (ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë¨)
        "personality_tags": ["impatient", "direct", "demanding", "angry"], 
        "communication_style": {"tone": "direct", "speed": "fast"},
        "category": "ë¶„ì‹¤/ë„ë‚œ",
        "persona_name": "í™”ë‚œ VIP ê³ ê°",
        "persona_description": "ë§¤ìš° ê¸‰í•œ ì„±ê²©ì´ë©°, ìì‹ ì˜ ì§€ìœ„ë¥¼ ì´ìš©í•˜ì—¬ ë¹¨ë¦¬ ì²˜ë¦¬í•´ì£¼ê¸°ë¥¼ ì›í•©ë‹ˆë‹¤. ì‘ì€ ì‹¤ìˆ˜ì—ë„ í™”ë¥¼ ëƒ…ë‹ˆë‹¤."
    }
    
    print(f"\n[ê³ ê° ì •ë³´] {customer_profile['name']} ({customer_profile['age_group']}, {customer_profile['grade']})")
    print(f"ì„±ê²©: {', '.join(customer_profile['personality_tags'])}")
    print(f"ì„¤ëª…: {customer_profile['persona_description']}")
    print("-" * 60)

    # 2. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    system_prompt = create_system_prompt(customer_profile, difficulty="beginner")
    print("\n[ìƒì„±ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸]")
    print(system_prompt)
    print("-" * 60)
    
    # 3. LLM í˜ë¥´ì†Œë‚˜ í…ŒìŠ¤íŠ¸
    print("\n[LLM í˜ë¥´ì†Œë‚˜ ì‘ë‹µ í…ŒìŠ¤íŠ¸]")
    
    # ì‹œë‚˜ë¦¬ì˜¤: ìƒë‹´ì›ì´ ì¸ì‚¬ë¥¼ ê±´ë„´
    user_inputs = [
        "ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ, í…Œë””ì¹´ë“œ ê³ ê°ì„¼í„° ìƒë‹´ì› ì´ì˜í¬ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
        "ê³ ê°ë‹˜, ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì‹œë©´ í™•ì¸í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
        "ì£„ì†¡í•˜ì§€ë§Œ ê·œì •ìƒ ì–´ë µìŠµë‹ˆë‹¤."
    ]
    
    current_system_prompt = system_prompt
    
    for user_input in user_inputs:
        print(f"\nğŸ—£ï¸ ìƒë‹´ì›: {user_input}")
        
        response = generate_text(
            prompt=user_input,
            system_prompt=current_system_prompt,
            temperature=0.7,
            max_tokens=200
        )
        
        print(f"ğŸ‘¤ ê³ ê°(í˜ë¥´ì†Œë‚˜): {response}")
        
        if not response:
            print("âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨")
            break

    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

if __name__ == "__main__":
    if test_model_connection():
        test_persona_generation_with_llm()
    else:
        print("\nâ›” ëª¨ë¸ ì—°ê²°ì— ì‹¤íŒ¨í•˜ì—¬ í˜ë¥´ì†Œë‚˜ í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
