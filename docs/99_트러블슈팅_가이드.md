# CALL:ACT 프로젝트 트러블슈팅 가이드

**작성일**: 2026-01-13  
**작성자**: CALL:ACT Team  
**버전**: v1.0

---

## 개요

이 문서는 CALL:ACT 프로젝트 개발 과정에서 발생한 주요 문제점과 해결 방법을 체계적으로 정리한 트러블슈팅 가이드입니다. 데이터 전처리, 데이터베이스 적재, API 연동, 파일 관리 등 전반적인 영역에서 겪은 문제와 해결 과정을 포함합니다.

---

## 목차

1. [데이터 전처리 관련 문제](#1-데이터-전처리-관련-문제)
2. [데이터베이스 적재 관련 문제](#2-데이터베이스-적재-관련-문제)
3. [네트워크 및 API 관련 문제](#3-네트워크-및-api-관련-문제)
4. [파일 관리 및 경로 관련 문제](#4-파일-관리-및-경로-관련-문제)
5. [키워드 사전 구축 관련 문제](#5-키워드-사전-구축-관련-문제)
6. [Git 및 버전 관리 관련 문제](#6-git-및-버전-관리-관련-문제)
7. [환경 설정 및 의존성 문제](#7-환경-설정-및-의존성-문제)

---

## 1. 데이터 전처리 관련 문제

### 1.1 데이터 저장 누락 문제

**문제 상황**:
- 24시간 동안 진행한 전처리 작업 결과가 제대로 저장되지 않음
- 하나카드 전처리 과정에서 일부 데이터가 누락됨

**원인 분석**:
- 파일 저장 로직에서 예외 처리 부족
- 중간 저장 지점이 없어 장시간 실행 중 오류 발생 시 데이터 손실
- 파일 경로 설정 오류로 인한 저장 실패

**해결 방법**:
1. **중간 저장 기능 추가**: 일정 간격으로 진행 상황 저장
2. **예외 처리 강화**: 파일 저장 시 try-except 블록 추가 및 상세 로그 기록
3. **백업 파일 생성**: 저장 전 기존 파일 백업 생성
4. **검증 로직 추가**: 저장 후 데이터 무결성 검증

**구현 예시**:
```python
def save_with_backup(data: List[Dict], output_path: Path, backup: bool = True):
    """백업을 포함한 안전한 저장 함수"""
    try:
        # 백업 생성
        if backup and output_path.exists():
            backup_path = output_path.with_suffix('.json.backup')
            shutil.copy2(output_path, backup_path)
        
        # 데이터 저장
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # 저장 검증
        with open(output_path, 'r', encoding='utf-8') as f:
            saved_data = json.load(f)
            if len(saved_data) != len(data):
                raise ValueError(f"저장된 데이터 개수 불일치: {len(saved_data)} != {len(data)}")
        
        print(f"[SUCCESS] {len(data)}개 항목 저장 완료: {output_path}")
        return True
    except Exception as e:
        print(f"[ERROR] 저장 실패: {e}")
        return False
```

**예방 조치**:
- 정기적인 중간 저장 (예: 100개 항목마다)
- 진행 상황 로그 파일 기록
- 저장 실패 시 자동 재시도 로직

---

### 1.2 LLM JSON 파싱 에러

**문제 상황**:
- OpenAI API 응답을 JSON으로 파싱하는 과정에서 에러 발생
- LLM이 JSON 형식이 아닌 응답을 반환하는 경우

**에러 메시지**:
```
[ERROR] LLM JSON parsing failed: Expecting value: line 1 column 1 (char 0)
```

**원인 분석**:
- LLM 응답이 코드 블록(```)으로 감싸져 있는 경우
- JSON 형식이 아닌 설명 텍스트가 포함된 경우
- 네트워크 오류로 인한 불완전한 응답

**해결 방법**:
1. **응답 전처리**: 코드 블록 제거 및 JSON 추출
2. **재시도 로직**: 파싱 실패 시 LLM 재호출
3. **에러 로그 저장**: 디버깅을 위한 상세 에러 로그 저장
4. **폴백 처리**: 파싱 실패 시 기본값 반환

**구현 예시**:
```python
def parse_llm_response(result_text: str) -> Dict:
    """LLM 응답 파싱 (에러 처리 포함)"""
    try:
        # 코드 블록 제거
        if result_text.startswith('```'):
            result_text = result_text.split('```')[1]
            if result_text.startswith('json'):
                result_text = result_text[4:]
            result_text = result_text.strip()
        
        # JSON 파싱
        result = json.loads(result_text)
        return result
    except json.JSONDecodeError as e:
        # 에러 로그 저장
        error_log_dir = Path(__file__).parent / 'error_logs'
        error_log_dir.mkdir(parents=True, exist_ok=True)
        error_file = error_log_dir / f"json_error_{int(time.time())}.txt"
        with open(error_file, 'w', encoding='utf-8') as f:
            f.write(f"=== JSON 파싱 에러 ===\n")
            f.write(f"에러: {e}\n")
            f.write(f"위치: line {e.lineno}, column {e.colno}\n")
            f.write(f"\n=== LLM 응답 전문 ===\n")
            f.write(result_text)
        
        # 폴백 처리
        return {
            "text": re.sub(r'▲+', '[개인정보]', original_text),
            "slot_types": [],
            "entity_mapping": {}
        }
```

**예방 조치**:
- 프롬프트에 JSON 형식 강제 명시
- 응답 검증 로직 추가
- 재시도 횟수 제한 설정 (MAX_RETRY_COUNT = 2)

---

### 1.3 검증 실패 및 재시도 문제

**문제 상황**:
- 마스킹 태깅 후 검증 과정에서 실패
- ▲ 기호가 남아있거나 잘못된 태그 형식 발견

**원인 분석**:
- LLM이 모든 ▲를 태그로 변환하지 못함
- 태그 형식 오류 ([타입#번호] 형식 불일치)
- Entity 일관성 문제 (같은 엔티티에 다른 번호 부여)

**해결 방법**:
1. **다단계 검증**: mask_check, tag_check, entity_check 순차 검증
2. **자동 재시도**: 검증 실패 시 LLM 재호출
3. **강제 처리**: 최종 검증 후에도 ▲가 남으면 [개인정보]로 강제 변환

**구현 예시**:
```python
def normalize_with_validation(text: str, max_retries: int = 2) -> str:
    """검증을 포함한 정규화 함수"""
    processed_text = text
    retry_count = 0
    
    while retry_count < max_retries:
        # LLM 처리
        result = classify_slots_with_llm(processed_text)
        processed_text = result['text']
        
        # 검증
        validation = validate_all(processed_text)
        
        if validation['valid']:
            break
        
        # 검증 실패 시 재시도
        retry_count += 1
        if not validation['mask_check']['valid']:
            # ▲ 잔존 시 재처리
            continue
        elif not validation['tag_check']['valid']:
            # 잘못된 태그 수정
            continue
    
    # 최종 검증 후에도 ▲가 남으면 강제 처리
    if '▲' in processed_text:
        processed_text = re.sub(r'▲+', '[개인정보]', processed_text)
    
    return processed_text
```

---

## 2. 데이터베이스 적재 관련 문제

### 2.1 RealDictCursor KeyError 문제

**문제 상황**:
- `psycopg2.extras.RealDictCursor` 사용 시 `KeyError: 0` 발생
- `SELECT EXISTS (...)` 쿼리 결과 접근 시 인덱스 오류

**에러 메시지**:
```
KeyError: 0
```

**원인 분석**:
- `RealDictCursor`는 결과를 딕셔너리로 반환하는데, 인덱스 접근 시도
- `EXISTS` 쿼리 결과에 별칭(alias)이 없어 키 접근 불가

**해결 방법**:
1. **별칭 추가**: `EXISTS` 결과에 명시적 별칭 부여
2. **딕셔너리 접근**: 인덱스 대신 키로 접근

**수정 전**:
```python
def check_table_exists(cursor, table_name: str) -> bool:
    cursor.execute("""
        SELECT EXISTS (
            SELECT FROM information_schema.tables 
            WHERE table_name = %s
        )
    """, (table_name,))
    result = cursor.fetchone()
    return result[0]  # KeyError 발생
```

**수정 후**:
```python
def check_table_exists(cursor, table_name: str, schema: str = 'public') -> bool:
    cursor.execute("""
        SELECT EXISTS (
            SELECT FROM information_schema.tables 
            WHERE table_schema = %s 
            AND table_name = %s
        ) as exists_result
    """, (schema, table_name))
    result = cursor.fetchone()
    return result['exists_result'] if result else False
```

---

### 2.2 pgvector 타입 캐스팅 오류

**문제 상황**:
- `vector` 타입을 `double precision[]`로 캐스팅 시도 시 오류 발생
- 임베딩 벡터 차원 확인 시 타입 변환 실패

**에러 메시지**:
```
psycopg2.errors.CannotCoerce: cannot cast type vector to double precision[]
```

**원인 분석**:
- PostgreSQL의 `vector` 타입은 직접 배열로 캐스팅 불가
- `pgvector` 확장의 타입 시스템 이해 부족

**해결 방법**:
1. **텍스트 변환 후 파싱**: `embedding::text`로 변환 후 Python에서 파싱
2. **차원 확인**: 벡터 길이를 문자열 파싱으로 확인

**수정 전**:
```python
cursor.execute("""
    SELECT array_length(embedding::double precision[], 1) as dim
    FROM service_guide_documents
    LIMIT 1
""")
```

**수정 후**:
```python
cursor.execute("""
    SELECT embedding::text as embedding_text
    FROM service_guide_documents
    WHERE embedding IS NOT NULL
    LIMIT 1
""")
result = cursor.fetchone()
if result:
    # 벡터 문자열 파싱: '[0.1, 0.2, ...]'
    embedding_str = result['embedding_text']
    embedding_list = json.loads(embedding_str)
    dimension = len(embedding_list)
```

---

### 2.3 컬럼 존재하지 않음 오류

**문제 상황**:
- 존재하지 않는 컬럼 참조로 인한 SQL 오류
- 스키마 변경 후 코드 업데이트 누락

**에러 메시지**:
```
psycopg2.errors.UndefinedColumn: column "keyword_id" does not exist
psycopg2.errors.UndefinedColumn: column "brand_type" does not exist
```

**원인 분석**:
- 실제 스키마와 코드의 컬럼명 불일치
- 테이블 구조 변경 후 관련 쿼리 미수정

**해결 방법**:
1. **스키마 확인**: DBeaver에서 실제 테이블 구조 확인
2. **컬럼명 수정**: 코드의 컬럼명을 실제 스키마에 맞게 수정
3. **동적 확인**: 컬럼 존재 여부를 동적으로 확인하는 함수 사용

**수정 예시**:
```python
# keyword_id → (canonical_keyword, category) 조합 사용
cursor.execute("""
    SELECT 
        COUNT(*) as total,
        COUNT(DISTINCT (canonical_keyword, category)) as keywords_with_synonyms
    FROM keyword_synonyms
""")

# brand_type → brand로 수정
cursor.execute("""
    SELECT 
        id,
        name,
        brand,  -- brand_type 대신 brand 사용
        CASE WHEN structured IS NOT NULL THEN 'Yes' ELSE 'No' END as has_structured
    FROM card_products
    LIMIT 3
""")
```

---

### 2.4 VARCHAR 컬럼에 집계 함수 사용 오류

**문제 상황**:
- VARCHAR 타입 컬럼에 `AVG()` 함수 사용 시 오류 발생
- `urgency` 컬럼이 문자열인데 평균 계산 시도

**에러 메시지**:
```
psycopg2.errors.UndefinedFunction: function avg(character varying) does not exist
```

**원인 분석**:
- `urgency` 컬럼이 'high', 'medium', 'low' 같은 문자열 값
- 숫자형 컬럼으로 오인하여 집계 함수 사용

**해결 방법**:
1. **분포 계산**: 집계 함수 대신 값별 개수 계산
2. **GROUP BY 사용**: 각 값의 분포 확인

**수정 전**:
```python
cursor.execute("""
    SELECT AVG(urgency) as avg_urgency
    FROM keyword_dictionary
""")
```

**수정 후**:
```python
cursor.execute("""
    SELECT urgency, COUNT(*) as count
    FROM keyword_dictionary
    GROUP BY urgency
    ORDER BY count DESC
""")
urgency_distribution = cursor.fetchall()
for row in urgency_distribution:
    urgency = row['urgency']
    count = row['count']
    print(f"  - {urgency}: {count:,}건")
```

---

### 2.5 데이터 적재 중 중복 키 오류

**문제 상황**:
- 동일한 ID로 데이터 적재 시도 시 UNIQUE 제약 조건 위반
- 중복 실행 시 데이터 중복 적재

**에러 메시지**:
```
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint
```

**해결 방법**:
1. **ON CONFLICT 사용**: PostgreSQL의 `ON CONFLICT DO UPDATE` 구문 활용
2. **중복 확인**: 적재 전 기존 데이터 확인

**구현 예시**:
```python
def insert_with_conflict_handling(cursor, table: str, data: Dict, id_field: str = 'id'):
    """중복 키 처리 포함 삽입"""
    columns = ', '.join(data.keys())
    placeholders = ', '.join(['%s'] * len(data))
    values = list(data.values())
    
    update_clause = ', '.join([f"{k} = EXCLUDED.{k}" for k in data.keys() if k != id_field])
    
    query = f"""
        INSERT INTO {table} ({columns})
        VALUES ({placeholders})
        ON CONFLICT ({id_field}) DO UPDATE SET {update_clause}
    """
    cursor.execute(query, values)
```

---

## 3. 네트워크 및 API 관련 문제

### 3.1 OpenAI API 타임아웃 오류

**문제 상황**:
- LLM API 호출 시 네트워크 타임아웃 발생
- 장시간 실행 중 일시적 네트워크 불안정

**에러 메시지**:
```
[ERROR] LLM processing failed (TimeoutError): Request timed out
[INFO] 네트워크 문제로 보입니다. 잠시 후 재시도해보세요.
```

**원인 분석**:
- 네트워크 연결 불안정
- API 서버 응답 지연
- 요청 타임아웃 설정 부족

**해결 방법**:
1. **재시도 로직**: 지수 백오프(exponential backoff) 방식 재시도
2. **타임아웃 설정**: 적절한 타임아웃 값 설정
3. **에러 분류**: 네트워크 오류와 다른 오류 구분

**구현 예시**:
```python
def call_llm_with_retry(prompt: str, max_retries: int = 3, timeout: int = 60):
    """재시도 로직 포함 LLM 호출"""
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=DEFAULT_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                timeout=timeout
            )
            return response
        except (TimeoutError, ConnectionError) as e:
            if attempt < max_retries - 1:
                wait_time = 5 * (2 ** attempt)  # 지수 백오프
                print(f"[RETRY {attempt + 1}/{max_retries}] 네트워크 오류, {wait_time}초 후 재시도...")
                time.sleep(wait_time)
            else:
                raise
        except Exception as e:
            # 네트워크 오류가 아닌 경우 즉시 재시도하지 않음
            raise
```

---

### 3.2 API Rate Limit 초과

**문제 상황**:
- OpenAI API 호출 빈도가 높아 Rate Limit 초과
- 일시적으로 API 사용 불가

**해결 방법**:
1. **요청 간 지연**: 각 요청 사이에 적절한 지연 시간 추가
2. **배치 처리**: 여러 요청을 배치로 묶어 처리
3. **Rate Limit 모니터링**: 사용량 추적 및 경고

**구현 예시**:
```python
# 요청 간 지연 설정
EMBEDDING_CONFIG = {
    "model": "text-embedding-3-small",
    "batch_size": 100,
    "request_delay": 0.5,  # 초 (API rate limit 고려)
    "max_retries": 3,
    "retry_delay": 5
}

def process_with_rate_limit(items: List, delay: float = 0.5):
    """Rate limit을 고려한 처리"""
    for item in tqdm(items):
        result = process_item(item)
        time.sleep(delay)  # 요청 간 지연
        yield result
```

---

### 3.3 HTML 응답 감지 (네트워크 에러)

**문제 상황**:
- API 응답이 JSON이 아닌 HTML로 반환되는 경우
- 네트워크 프록시나 방화벽에 의한 차단

**해결 방법**:
1. **응답 타입 확인**: Content-Type 헤더 확인
2. **HTML 감지**: 응답이 HTML인지 확인 후 재시도

**구현 예시**:
```python
def validate_api_response(response):
    """API 응답 유효성 검증"""
    if response.headers.get('content-type', '').startswith('text/html'):
        raise ValueError("HTML 응답 감지 (네트워크 에러)")
    
    try:
        data = response.json()
        return data
    except json.JSONDecodeError:
        raise ValueError("JSON 파싱 실패")
```

---

## 4. 파일 관리 및 경로 관련 문제

### 4.1 파일 경로 찾기 실패

**문제 상황**:
- 프로덕션 경로와 개발 경로가 다를 때 파일을 찾지 못함
- 상대 경로 설정 오류

**에러 메시지**:
```
[ERROR] 입력 파일이 없습니다: C:\...\teddycard_service_guides.json
[ERROR] 키워드 사전 파일이 없습니다: keywords_dict.json
```

**원인 분석**:
- 하드코딩된 경로 사용
- 환경별 경로 차이 미고려
- 파일명 변경 후 경로 업데이트 누락

**해결 방법**:
1. **우선순위 기반 파일 찾기**: 프로덕션 경로 우선, 개발 경로 대체
2. **동적 경로 설정**: 환경 변수 기반 경로 설정
3. **파일 존재 확인**: 파일 사용 전 존재 여부 확인

**구현 예시**:
```python
def find_data_file(filename: str, prod_path: Path, dev_path: Path) -> Optional[Path]:
    """프로덕션 우선, 개발 환경 대체 파일 찾기"""
    # 1. 프로덕션 경로 확인
    prod_file = prod_path / filename
    if prod_file.exists():
        return prod_file
    
    # 2. 개발 경로 확인
    dev_file = dev_path / filename
    if dev_file.exists():
        return dev_file
    
    # 3. 파일 없음
    print(f"[ERROR] 파일을 찾을 수 없습니다: {filename}")
    print(f"  프로덕션 경로: {prod_path}")
    print(f"  개발 경로: {dev_path}")
    return None
```

---

### 4.2 Git LFS 파일 다운로드 실패

**문제 상황**:
- Git LFS로 관리되는 큰 파일이 다운로드되지 않음
- 파일이 포인터만 보이고 실제 내용이 비어있음

**원인 분석**:
- Git LFS 미설치
- `git lfs pull` 명령어 미실행
- LFS 초기화 누락

**해결 방법**:
1. **Git LFS 설치 확인**: `git lfs version` 명령어로 확인
2. **LFS 초기화**: `git lfs install` 실행
3. **LFS 파일 다운로드**: `git lfs pull` 실행

**실행 순서**:
```bash
# 1. Git LFS 설치 확인
git lfs version

# 2. Git LFS 초기화 (처음 한 번만)
git lfs install

# 3. 각 레포지토리에서 LFS 파일 다운로드
cd data-preprocessing
git lfs pull
```

**참고**: [Git LFS 설치 및 사용 가이드](../00_git/Git_LFS_설치_및_사용_가이드.md)

---

### 4.3 파일 인코딩 문제

**문제 상황**:
- 한글 파일명이 깨져서 표시됨
- 파일 읽기/쓰기 시 인코딩 오류

**해결 방법**:
1. **UTF-8 인코딩 명시**: 파일 열기 시 `encoding='utf-8'` 지정
2. **경로 처리**: `pathlib.Path` 사용으로 플랫폼 독립적 경로 처리

**구현 예시**:
```python
# 파일 읽기
with open(file_path, 'r', encoding='utf-8') as f:
    data = json.load(f)

# 파일 쓰기
with open(file_path, 'w', encoding='utf-8') as f:
    json.dump(data, f, ensure_ascii=False, indent=2)
```

---

## 5. 키워드 사전 구축 관련 문제

### 5.1 키워드 우선순위 부재

**문제 상황**:
- "분실", "정지" 같은 긴급 키워드와 일반 키워드가 동일하게 처리됨
- RAG 검색 시 중요한 정보가 뒤로 밀림

**예시**:
- "일본 여행 중 카드 분실" → "분실"이 가장 중요한 키워드인데 우선순위 반영 안됨

**해결 방법**:
1. **우선순위 필드 추가**: 키워드 사전에 `priority` 필드 추가 (1-10)
2. **긴급성 필드 추가**: `urgency` 필드 추가 ('high', 'medium', 'low')
3. **우선순위 기반 정렬**: 검색 시 우선순위와 긴급성 고려

**구현 예시**:
```python
{
  "분실": {
    "categories": [
      {"category": "분실도난", "priority": 10, "urgency": "high"}
    ]
  }
}
```

---

### 5.2 동의어/유의어 처리 부재

**문제 상황**:
- "잃어버림", "분실신고", "카드분실" 등이 별도 키워드로 처리
- STT가 다른 표현을 사용하면 키워드 매칭 실패

**해결 방법**:
1. **Thesaurus 기반 동의어 사전 구축**: 표준 키워드(Canonical) + 동의어(Synonyms)
2. **동의어 테이블 생성**: `keyword_synonyms` 테이블에 동의어 매핑 저장

**구현 예시**:
```python
{
  "분실": {
    "synonyms": ["잃어버림", "분실신고", "카드분실"],
    "variations": ["분실했어요", "분실됐어요"]
  }
}
```

---

### 5.3 복합 키워드 우선 처리 부재

**문제 상황**:
- "해외 카드 분실" > "카드 분실" > "분실" 순서로 우선순위 처리 안됨
- 더 구체적인 키워드가 일반 키워드와 동일하게 처리됨

**해결 방법**:
1. **복합 키워드 패턴 인식**: 긴 패턴부터 매칭하여 더 구체적인 키워드 우선
2. **우선순위 부여**: 복합 키워드에 더 높은 우선순위 부여

**구현 예시**:
```python
COMPOUND_KEYWORDS = {
    "해외 카드 분실": {"priority": 10, "category": "분실도난"},
    "카드 분실": {"priority": 9, "category": "분실도난"},
    "분실": {"priority": 8, "category": "분실도난"}
}
```

---

### 5.4 중복 키워드 카테고리 선택 문제

**문제 상황**:
- "도난"이 8개 카테고리에 속할 때 어떤 것을 선택할지 불명확
- 맥락 정보 부족으로 잘못된 카테고리 선택

**해결 방법**:
1. **맥락 기반 카테고리 선택**: 주변 키워드와 문맥을 고려한 카테고리 선택
2. **맥락 규칙 정의**: `context_rules`를 통한 선택 로직 정의

**구현 예시**:
```python
{
  "도난": {
    "categories": [
      {"category": "분실도난", "priority": 10, "context": ["카드", "분실", "신고"]},
      {"category": "결제", "priority": 3, "context": ["도난 카드", "결제 거부"]}
    ],
    "ambiguity_resolution": {
      "default": "분실도난",
      "context_rules": [
        {"if": ["결제", "거부"], "then": "결제"},
        {"if": ["신고", "정지"], "then": "분실도난"}
      ]
    }
  }
}
```

---

## 6. Git 및 버전 관리 관련 문제

### 6.1 서브모듈 동기화 문제

**문제 상황**:
- 서브모듈이 최신 상태가 아님
- 팀 레포지토리 변경사항이 반영되지 않음

**해결 방법**:
1. **서브모듈 업데이트**: `git submodule update --remote`
2. **서브모듈 커밋**: 서브모듈 변경사항 커밋 후 메인 레포지토리 업데이트

**참고**: [서브모듈 동기화 실행 가이드](../00_git/서브모듈_동기화_실행_가이드.md)

---

### 6.2 Git LFS 할당량 초과

**문제 상황**:
- Git LFS 저장소의 데이터 할당량 초과
- 파일 업로드 실패

**에러 메시지**:
```
batch response: This repository is over its data quota
```

**해결 방법**:
1. **불필요한 파일 제거**: 큰 파일 중 불필요한 것 제거
2. **할당량 증가 요청**: Git LFS 저장소 관리자에게 할당량 증가 요청
3. **로컬 파일 제외**: `.gitignore`에 로컬에서만 필요한 파일 추가

---

### 6.3 브랜치 병합 충돌

**문제 상황**:
- 브랜치 병합 시 충돌 발생
- 팀원과의 변경사항 충돌

**해결 방법**:
1. **최신 상태로 업데이트**: 병합 전 `git pull` 실행
2. **충돌 해결**: 충돌 파일 수동 편집 후 커밋
3. **테스트**: 병합 후 기능 테스트

---

## 7. 환경 설정 및 의존성 문제

### 7.1 LLM 모델 설정 오류

**문제 상황**:
- 잘못된 모델명 사용 (예: `gpt-4.1-mini` → 실제로는 `gpt-4o-mini`)
- 환경 변수 설정 누락

**해결 방법**:
1. **올바른 모델명 사용**: OpenAI 공식 모델명 확인
2. **환경 변수 설정**: `.env` 파일에 `LLM_MODEL` 설정
3. **기본값 설정**: 코드에 기본 모델명 설정

**구현 예시**:
```python
LLM_CONFIG = {
    "model": os.getenv("LLM_MODEL", "gpt-4o-mini"),  # 올바른 모델명
    "temperature": float(os.getenv("LLM_TEMPERATURE", "0.3")),
    "max_tokens": int(os.getenv("LLM_MAX_TOKENS", "2000"))
}
```

---

### 7.2 환경 변수 로드 실패

**문제 상황**:
- `.env` 파일이 로드되지 않음
- API 키 등 환경 변수를 찾을 수 없음

**에러 메시지**:
```
[WARNING] OPENAI_API_KEY not found. Skipping LLM processing.
```

**해결 방법**:
1. **`.env` 파일 위치 확인**: 스크립트 실행 위치 기준 상대 경로 확인
2. **환경 변수 로드**: `python-dotenv`를 사용한 명시적 로드
3. **경로 지정**: 절대 경로로 `.env` 파일 지정

**구현 예시**:
```python
from dotenv import load_dotenv
from pathlib import Path

# .env 파일 로드 (프로젝트 루트 기준)
env_path = Path(__file__).parent.parent.parent / '.env'
load_dotenv(env_path)
```

---

### 7.3 의존성 패키지 누락

**문제 상황**:
- 필요한 Python 패키지가 설치되지 않음
- 버전 불일치로 인한 오류

**해결 방법**:
1. **requirements.txt 확인**: 필요한 패키지 목록 확인
2. **패키지 설치**: `pip install -r requirements.txt`
3. **Conda 환경 사용**: 프로젝트 전용 Conda 환경 사용

**실행 순서**:
```bash
# Conda 환경 활성화
conda activate final_env

# 패키지 설치
pip install -r requirements.txt
```

---

## 8. 종합 해결 전략

### 8.1 문제 발생 시 체크리스트

1. **에러 메시지 확인**: 정확한 에러 메시지와 스택 트레이스 확인
2. **로그 파일 확인**: 에러 로그 파일에서 상세 정보 확인
3. **환경 확인**: 환경 변수, 파일 경로, 의존성 확인
4. **문서 참고**: 관련 문서에서 해결 방법 확인
5. **백업 확인**: 문제 발생 전 백업 파일 존재 여부 확인

### 8.2 예방 조치

1. **정기적인 백업**: 중요한 데이터 정기 백업
2. **에러 로깅**: 상세한 에러 로그 기록
3. **검증 로직**: 데이터 무결성 검증 로직 추가
4. **테스트**: 변경사항 적용 전 테스트
5. **문서화**: 문제 해결 과정 문서화

### 8.3 디버깅 도구

1. **DBeaver**: 데이터베이스 구조 및 데이터 확인
2. **에러 로그 파일**: `error_logs/` 디렉토리의 상세 에러 로그
3. **진행 상황 로그**: 전처리 과정의 진행 상황 로그
4. **검증 스크립트**: `06_verify_teddycard_load.py` 등 검증 스크립트

---

## 9. 참고 문서

- [통합 DB 설정 가이드](./03_DB_적재_배포/11_통합_DB_설정_가이드.md)
- [DB 스키마 에러 해결 가이드](./03_DB_적재_배포/06_DB_스키마_에러_해결_가이드.md)
- [Teddycard 데이터 검증](./03_DB_적재_배포/08_Teddycard_데이터_검증.md)
- [Git LFS 설치 및 사용 가이드](../00_git/Git_LFS_설치_및_사용_가이드.md)
- [서브모듈 동기화 실행 가이드](../00_git/서브모듈_동기화_실행_가이드.md)

---

**마지막 업데이트**: 2026-01-13
